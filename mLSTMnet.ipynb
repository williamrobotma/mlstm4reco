{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mLSTMnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williamrobotma/mlstm4reco/blob/master/mLSTMnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/maciejkula/spotlight.git@master#egg=spotlight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIzylJuVVl_p",
        "outputId": "87b28f60-d1ea-4b2a-d273-ea330d0dbbe6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spotlight\n",
            "  Cloning https://github.com/maciejkula/spotlight.git (to revision master) to /tmp/pip-install-i9fwtbid/spotlight_8e5baf80a739438ba62a27be95cd8707\n",
            "  Running command git clone -q https://github.com/maciejkula/spotlight.git /tmp/pip-install-i9fwtbid/spotlight_8e5baf80a739438ba62a27be95cd8707\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spotlight) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->spotlight) (3.10.0.2)\n",
            "Building wheels for collected packages: spotlight\n",
            "  Building wheel for spotlight (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spotlight: filename=spotlight-0.1.6-py3-none-any.whl size=33929 sha256=3185e6b1b10fc90f10ec1e58c97b013a8f2fb293416fa15c0e201e0a8ded701a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6m98rkf2/wheels/d8/8b/76/508de2a4f4d2dc273e47fd34f78bda690f62661bf9d1e43bb1\n",
            "Successfully built spotlight\n",
            "Installing collected packages: spotlight\n",
            "Successfully installed spotlight-0.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Sq8uv4c9S3-A"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.nn import Parameter\n",
        "from torch.nn.modules.rnn import RNNBase, LSTMCell\n",
        "from torch.nn import functional as F\n",
        "from torch import nn\n",
        "import scipy.stats as ss\n",
        "\n",
        "import scipy.sparse as sp\n",
        "\n",
        "from sklearn.utils import murmurhash3_32\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import pickle\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "51KGxlUHT-i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ZeroEmbedding(nn.Embedding):\n",
        "    \"\"\"\n",
        "    Embedding layer that initialises its values\n",
        "    to using a normal variable scaled by the inverse\n",
        "    of the embedding dimension.\n",
        "    Used for biases.\n",
        "    \"\"\"\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"\n",
        "        Initialize parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        self.weight.data.zero_()\n",
        "        if self.padding_idx is not None:\n",
        "            self.weight.data[self.padding_idx].fill_(0)\n",
        "\n",
        "\n",
        "class ScaledEmbedding(nn.Embedding):\n",
        "    \"\"\"\n",
        "    Embedding layer that initialises its values\n",
        "    to using a normal variable scaled by the inverse\n",
        "    of the embedding dimension.\n",
        "    \"\"\"\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"\n",
        "        Initialize parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        self.weight.data.normal_(0, 1.0 / self.embedding_dim)\n",
        "        if self.padding_idx is not None:\n",
        "            self.weight.data[self.padding_idx].fill_(0)"
      ],
      "metadata": {
        "id": "z7LqqL_8rrHJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class mLSTM(RNNBase):\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        super(mLSTM, self).__init__(\n",
        "            mode='LSTM', input_size=input_size, hidden_size=hidden_size,\n",
        "                 num_layers=1, bias=bias, batch_first=True,\n",
        "                 dropout=0, bidirectional=False)\n",
        "\n",
        "        w_im = torch.Tensor(hidden_size, input_size)\n",
        "        w_hm = torch.Tensor(hidden_size, hidden_size)\n",
        "        b_im = torch.Tensor(hidden_size)\n",
        "        b_hm = torch.Tensor(hidden_size)\n",
        "        self.w_im = Parameter(w_im)\n",
        "        self.b_im = Parameter(b_im)\n",
        "        self.w_hm = Parameter(w_hm)\n",
        "        self.b_hm = Parameter(b_hm)\n",
        "\n",
        "        self.lstm_cell = LSTMCell(input_size, hidden_size, bias)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, hx):\n",
        "        n_batch, n_seq, n_feat = input.size()\n",
        "\n",
        "        hx, cx = hx\n",
        "        steps = [cx.unsqueeze(1)]\n",
        "        for seq in range(n_seq):\n",
        "            mx = F.linear(input[:, seq, :], self.w_im, self.b_im) * F.linear(hx, self.w_hm, self.b_hm)\n",
        "            hx = (mx, cx)\n",
        "            hx, cx = self.lstm_cell(input[:, seq, :], hx)\n",
        "            steps.append(cx.unsqueeze(1))\n",
        "\n",
        "        return torch.cat(steps, dim=1)"
      ],
      "metadata": {
        "id": "RUIGoeGcT9mO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PADDING_IDX = 0\n",
        "\n",
        "\n",
        "class mLSTMNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Module representing users through running a recurrent neural network\n",
        "    over the sequence, using the hidden state at each timestep as the\n",
        "    sequence representation, a'la [2]_\n",
        "    During training, representations for all timesteps of the sequence are\n",
        "    computed in one go. Loss functions using the outputs will therefore\n",
        "    be aggregating both across the minibatch and across time in the sequence.\n",
        "    Parameters\n",
        "    ----------\n",
        "    num_items: int\n",
        "        Number of items to be represented.\n",
        "    embedding_dim: int, optional\n",
        "        Embedding dimension of the embedding layer, and the number of hidden\n",
        "        units in the LSTM layer.\n",
        "    item_embedding_layer: an embedding layer, optional\n",
        "        If supplied, will be used as the item embedding layer\n",
        "        of the network.\n",
        "    References\n",
        "    ----------\n",
        "    .. [2] Hidasi, Balazs, et al. \"Session-based recommendations with\n",
        "       recurrent neural networks.\" arXiv preprint arXiv:1511.06939 (2015).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_items, embedding_dim=32,\n",
        "                 item_embedding_layer=None, sparse=False):\n",
        "\n",
        "        super(mLSTMNet, self).__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        if item_embedding_layer is not None:\n",
        "            self.item_embeddings = item_embedding_layer\n",
        "        else:\n",
        "            self.item_embeddings = ScaledEmbedding(num_items, embedding_dim,\n",
        "                                                   padding_idx=PADDING_IDX,\n",
        "                                                   sparse=sparse)\n",
        "\n",
        "        self.item_biases = ZeroEmbedding(num_items, 1, sparse=sparse,\n",
        "                                         padding_idx=PADDING_IDX)\n",
        "\n",
        "        h_init = torch.zeros(embedding_dim)\n",
        "        h_init.normal_(0, 1.0 / self.embedding_dim)\n",
        "        self.h_init = nn.Parameter(h_init, requires_grad=True)\n",
        "\n",
        "        self.mlstm = mLSTM(input_size=embedding_dim,\n",
        "                           hidden_size=embedding_dim)\n",
        "\n",
        "    def user_representation(self, item_sequences):\n",
        "        \"\"\"\n",
        "        Compute user representation from a given sequence.\n",
        "        Returns\n",
        "        -------\n",
        "        tuple (all_representations, final_representation)\n",
        "            The first element contains all representations from step\n",
        "            -1 (no items seen) to t - 1 (all but the last items seen).\n",
        "            The second element contains the final representation\n",
        "            at step t (all items seen). This final state can be used\n",
        "            for prediction or evaluation.\n",
        "        \"\"\"\n",
        "        # Make the embedding dimension the channel dimension\n",
        "        sequence_embeddings = self.item_embeddings(item_sequences)\n",
        "\n",
        "        # pad from left with initial state\n",
        "        batch_size = sequence_embeddings.size()[0]\n",
        "        embedding_dim = self.h_init.size()[0]\n",
        "        seq_start = self.h_init.expand(batch_size, embedding_dim)\n",
        "\n",
        "        user_representations = self.mlstm(sequence_embeddings, (seq_start, seq_start))\n",
        "\n",
        "        user_representations = user_representations.permute(0, 2, 1)\n",
        "\n",
        "        return user_representations[:, :, :-1], user_representations[:, :, -1]\n",
        "\n",
        "    def forward(self, user_representations, targets):\n",
        "        \"\"\"\n",
        "        Compute predictions for target items given user representations.\n",
        "        Parameters\n",
        "        ----------\n",
        "        user_representations: tensor\n",
        "            Result of the user_representation_method.\n",
        "        targets: tensor\n",
        "            A minibatch of item sequences of shape\n",
        "            (minibatch_size, sequence_length).\n",
        "        Returns\n",
        "        -------\n",
        "        predictions: tensor\n",
        "            of shape (minibatch_size, sequence_length)\n",
        "        \"\"\"\n",
        "\n",
        "        target_embedding = (self.item_embeddings(targets)\n",
        "                            .permute(0, 2, 1)\n",
        "                            .squeeze())\n",
        "        target_bias = self.item_biases(targets).squeeze()\n",
        "\n",
        "        dot = ((user_representations * target_embedding)\n",
        "               .sum(1)\n",
        "               .squeeze())\n",
        "\n",
        "        return target_bias + dot"
      ],
      "metadata": {
        "id": "BDgV-CJ6UGiF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spotlight Stuff"
      ],
      "metadata": {
        "id": "oOKq8EyDr9QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Spotlight Losses\n",
        "\n",
        "def hinge_loss(positive_predictions, negative_predictions, mask=None):\n",
        "    \"\"\"\n",
        "    Hinge pairwise loss function.\n",
        "    Parameters\n",
        "    ----------\n",
        "    positive_predictions: tensor\n",
        "        Tensor containing predictions for known positive items.\n",
        "    negative_predictions: tensor\n",
        "        Tensor containing predictions for sampled negative items.\n",
        "    mask: tensor, optional\n",
        "        A binary tensor used to zero the loss from some entries\n",
        "        of the loss tensor.\n",
        "    Returns\n",
        "    -------\n",
        "    loss, float\n",
        "        The mean value of the loss function.\n",
        "    \"\"\"\n",
        "\n",
        "    loss = torch.clamp(negative_predictions -\n",
        "                       positive_predictions +\n",
        "                       1.0, 0.0)\n",
        "\n",
        "    if mask is not None:\n",
        "        mask = mask.float()\n",
        "        loss = loss * mask\n",
        "        return loss.sum() / mask.sum()\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "def adaptive_hinge_loss(positive_predictions, negative_predictions, mask=None):\n",
        "    \"\"\"\n",
        "    Adaptive hinge pairwise loss function. Takes a set of predictions\n",
        "    for implicitly negative items, and selects those that are highest,\n",
        "    thus sampling those negatives that are closes to violating the\n",
        "    ranking implicit in the pattern of user interactions.\n",
        "    Approximates the idea of weighted approximate-rank pairwise loss\n",
        "    introduced in [2]_\n",
        "    Parameters\n",
        "    ----------\n",
        "    positive_predictions: tensor\n",
        "        Tensor containing predictions for known positive items.\n",
        "    negative_predictions: tensor\n",
        "        Iterable of tensors containing predictions for sampled negative items.\n",
        "        More tensors increase the likelihood of finding ranking-violating\n",
        "        pairs, but risk overfitting.\n",
        "    mask: tensor, optional\n",
        "        A binary tensor used to zero the loss from some entries\n",
        "        of the loss tensor.\n",
        "    Returns\n",
        "    -------\n",
        "    loss, float\n",
        "        The mean value of the loss function.\n",
        "    References\n",
        "    ----------\n",
        "    .. [2] Weston, Jason, Samy Bengio, and Nicolas Usunier. \"Wsabie:\n",
        "       Scaling up to large vocabulary image annotation.\" IJCAI.\n",
        "       Vol. 11. 2011.\n",
        "    \"\"\"\n",
        "\n",
        "    highest_negative_predictions, _ = torch.max(negative_predictions, 0)\n",
        "\n",
        "    return hinge_loss(positive_predictions, highest_negative_predictions.squeeze(), mask=mask)\n",
        "\n",
        "### Spotlight eval\n",
        "\n",
        "def sequence_mrr_score(predictions, targets, exclude_preceding=False):\n",
        "    \"\"\"\n",
        "    Compute mean reciprocal rank (MRR) scores. Each sequence\n",
        "    in test is split into two parts: the first part, containing\n",
        "    all but the last elements, is used to predict the last element.\n",
        "    The reciprocal rank of the last element is returned for each\n",
        "    sequence.\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: fitted instance of a recommender model\n",
        "        The model to evaluate.\n",
        "    test: :class:`spotlight.interactions.SequenceInteractions`\n",
        "        Test interactions.\n",
        "    exclude_preceding: boolean, optional\n",
        "        When true, items already present in the sequence will\n",
        "        be excluded from evaluation.\n",
        "    Returns\n",
        "    -------\n",
        "    mrr scores: numpy array of shape (num_users,)\n",
        "        Array of MRR scores for each sequence in test.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    mrrs = []\n",
        "\n",
        "    for i in range(len(targets)):\n",
        "\n",
        "        \n",
        "\n",
        "        if exclude_preceding:\n",
        "            predictions[i][sequences[i]] = np.finfo(np.float32).max\n",
        "\n",
        "        mrr = (1.0 / ss.rankdata(predictions[i])[targets[i]]).mean()\n",
        "\n",
        "        mrrs.append(mrr)\n",
        "\n",
        "    return np.array(mrrs)\n",
        "\n",
        "### Spotlight Implicit Sequence functions\n",
        "\n",
        "def get_negative_prediction(shape, user_representation, net, random_state=None):\n",
        "    if random_state is None:\n",
        "        random_state = np.random.RandomState()\n",
        "\n",
        "    negative_items = random_state.randint(0, num_items, shape, dtype=np.int64)\n",
        "    negative_var = torch.from_numpy(negative_items)\n",
        "    negative_var = negative_var.to(device)\n",
        "\n",
        "    negative_prediction = net(user_representation, negative_var)\n",
        "\n",
        "    return negative_prediction\n",
        "\n",
        "def get_multiple_negative_predictions(shape, user_representation, net,n=5, random_state=None):\n",
        "\n",
        "    if random_state is None:\n",
        "        random_state = np.random.RandomState()\n",
        "\n",
        "    batch_size, sliding_window = shape\n",
        "    size = (n,) + (1,) * (user_representation.dim() - 1)\n",
        "    negative_prediction = get_negative_prediction(\n",
        "        (n * batch_size, sliding_window),\n",
        "        user_representation.repeat(*size), net)\n",
        "\n",
        "    return negative_prediction.view(n, batch_size, sliding_window)\n",
        "    \n",
        "\n",
        "\n",
        "def check_input(item_ids, num_items):\n",
        "\n",
        "    if isinstance(item_ids, int):\n",
        "        item_id_max = item_ids\n",
        "    else:\n",
        "        item_id_max = item_ids.max()\n",
        "\n",
        "    if item_id_max >= num_items:\n",
        "        raise ValueError('Maximum item id greater '\n",
        "                          'than number of items in model.')\n",
        "\n",
        "def predict(net, sequences, num_items,item_ids=None):\n",
        "    \"\"\"\n",
        "    Make predictions: given a sequence of interactions, predict\n",
        "    the next item in the sequence.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    sequences: array, (1 x max_sequence_length)\n",
        "        Array containing the indices of the items in the sequence.\n",
        "    item_ids: array (num_items x 1), optional\n",
        "        Array containing the item ids for which prediction scores\n",
        "        are desired. If not supplied, predictions for all items\n",
        "        will be computed.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "\n",
        "    predictions: array\n",
        "        Predicted scores for all items in item_ids.\n",
        "    \"\"\"\n",
        "\n",
        "    net.train(False)\n",
        "\n",
        "    sequences = np.atleast_2d(sequences)\n",
        "\n",
        "    if item_ids is None:\n",
        "        item_ids = np.arange(num_items).reshape(-1, 1)\n",
        "\n",
        "\n",
        "    check_input(item_ids, num_items)\n",
        "    check_input(sequences, num_items)\n",
        "\n",
        "    sequences = torch.from_numpy(sequences.astype(np.int64).reshape(1, -1))\n",
        "    item_ids = torch.from_numpy(item_ids.astype(np.int64))\n",
        "\n",
        "    sequence_var = sequences.to(device)\n",
        "    item_var = item_ids.to(device)\n",
        "\n",
        "    _, sequence_representations = net.user_representation(sequence_var)\n",
        "    size = (len(item_var),) + sequence_representations.size()[1:]\n",
        "    out = net(sequence_representations.expand(*size),\n",
        "                    item_var)\n",
        "\n",
        "    return out.cpu().detach().numpy().flatten()\n",
        "\n",
        "## seems like this functionality could be replace by a dataloader?\n",
        "\n",
        "def shuffle(*arrays, **kwargs):\n",
        "\n",
        "    random_state = kwargs.get('random_state')\n",
        "\n",
        "    if len(set(len(x) for x in arrays)) != 1:\n",
        "        raise ValueError('All inputs to shuffle must have '\n",
        "                         'the same length.')\n",
        "\n",
        "    if random_state is None:\n",
        "        random_state = np.random.RandomState()\n",
        "\n",
        "    shuffle_indices = np.arange(len(arrays[0]))\n",
        "    random_state.shuffle(shuffle_indices)\n",
        "\n",
        "    if len(arrays) == 1:\n",
        "        return arrays[0][shuffle_indices]\n",
        "    else:\n",
        "        return tuple(x[shuffle_indices] for x in arrays)\n",
        "\n",
        "\n",
        "def minibatch(*tensors, **kwargs):\n",
        "\n",
        "    batch_size = kwargs.get('batch_size', 128)\n",
        "\n",
        "    if len(tensors) == 1:\n",
        "        tensor = tensors[0]\n",
        "        for i in range(0, len(tensor), batch_size):\n",
        "            yield tensor[i:i + batch_size]\n",
        "    else:\n",
        "        for i in range(0, len(tensors[0]), batch_size):\n",
        "            yield tuple(x[i:i + batch_size] for x in tensors)"
      ],
      "metadata": {
        "id": "kXcYsEbBm_QL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "xEB3HreLsDHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactions Data"
      ],
      "metadata": {
        "id": "8-wM63wXsZLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _sliding_window(tensor, window_size, step_size=1):\n",
        "\n",
        "    for i in range(len(tensor), 0, -step_size):\n",
        "        yield tensor[max(i - window_size, 0):i]\n",
        "\n",
        "\n",
        "def _generate_sequences(user_ids, item_ids,\n",
        "                        indices,\n",
        "                        max_sequence_length,\n",
        "                        step_size):\n",
        "\n",
        "    for i in range(len(indices)):\n",
        "\n",
        "        start_idx = indices[i]\n",
        "\n",
        "        if i >= len(indices) - 1:\n",
        "            stop_idx = None\n",
        "        else:\n",
        "            stop_idx = indices[i + 1]\n",
        "\n",
        "        for seq in _sliding_window(item_ids[start_idx:stop_idx],\n",
        "                                   max_sequence_length,\n",
        "                                   step_size):\n",
        "\n",
        "            yield (user_ids[i], seq)\n",
        "\n",
        "\n",
        "class Interactions(object):\n",
        "    \"\"\"\n",
        "    Interactions object. Contains (at a minimum) pair of user-item\n",
        "    interactions, but can also be enriched with ratings, timestamps,\n",
        "    and interaction weights.\n",
        "    For *implicit feedback* scenarios, user ids and item ids should\n",
        "    only be provided for user-item pairs where an interaction was\n",
        "    observed. All pairs that are not provided are treated as missing\n",
        "    observations, and often interpreted as (implicit) negative\n",
        "    signals.\n",
        "    For *explicit feedback* scenarios, user ids, item ids, and\n",
        "    ratings should be provided for all user-item-rating triplets\n",
        "    that were observed in the dataset.\n",
        "    Parameters\n",
        "    ----------\n",
        "    user_ids: array of np.int32\n",
        "        array of user ids of the user-item pairs\n",
        "    item_ids: array of np.int32\n",
        "        array of item ids of the user-item pairs\n",
        "    ratings: array of np.float32, optional\n",
        "        array of ratings\n",
        "    timestamps: array of np.int32, optional\n",
        "        array of timestamps\n",
        "    weights: array of np.float32, optional\n",
        "        array of weights\n",
        "    num_users: int, optional\n",
        "        Number of distinct users in the dataset.\n",
        "        Must be larger than the maximum user id\n",
        "        in user_ids.\n",
        "    num_items: int, optional\n",
        "        Number of distinct items in the dataset.\n",
        "        Must be larger than the maximum item id\n",
        "        in item_ids.\n",
        "    Attributes\n",
        "    ----------\n",
        "    user_ids: array of np.int32\n",
        "        array of user ids of the user-item pairs\n",
        "    item_ids: array of np.int32\n",
        "        array of item ids of the user-item pairs\n",
        "    ratings: array of np.float32, optional\n",
        "        array of ratings\n",
        "    timestamps: array of np.int32, optional\n",
        "        array of timestamps\n",
        "    weights: array of np.float32, optional\n",
        "        array of weights\n",
        "    num_users: int, optional\n",
        "        Number of distinct users in the dataset.\n",
        "    num_items: int, optional\n",
        "        Number of distinct items in the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, user_ids, item_ids,\n",
        "                 ratings=None,\n",
        "                 timestamps=None,\n",
        "                 weights=None,\n",
        "                 num_users=None,\n",
        "                 num_items=None):\n",
        "\n",
        "        self.num_users = num_users or int(user_ids.max() + 1)\n",
        "        self.num_items = num_items or int(item_ids.max() + 1)\n",
        "\n",
        "        self.user_ids = user_ids\n",
        "        self.item_ids = item_ids\n",
        "        self.ratings = ratings\n",
        "        self.timestamps = timestamps\n",
        "        self.weights = weights\n",
        "\n",
        "        self._check()\n",
        "\n",
        "    def __repr__(self):\n",
        "\n",
        "        return ('<Interactions dataset ({num_users} users x {num_items} items '\n",
        "                'x {num_interactions} interactions)>'\n",
        "                .format(\n",
        "                    num_users=self.num_users,\n",
        "                    num_items=self.num_items,\n",
        "                    num_interactions=len(self)\n",
        "                ))\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.user_ids)\n",
        "\n",
        "    def _check(self):\n",
        "\n",
        "        if self.user_ids.max() >= self.num_users:\n",
        "            raise ValueError('Maximum user id greater '\n",
        "                             'than declared number of users.')\n",
        "        if self.item_ids.max() >= self.num_items:\n",
        "            raise ValueError('Maximum item id greater '\n",
        "                             'than declared number of items.')\n",
        "\n",
        "        num_interactions = len(self.user_ids)\n",
        "\n",
        "        for name, value in (('item IDs', self.item_ids),\n",
        "                            ('ratings', self.ratings),\n",
        "                            ('timestamps', self.timestamps),\n",
        "                            ('weights', self.weights)):\n",
        "\n",
        "            if value is None:\n",
        "                continue\n",
        "\n",
        "            if len(value) != num_interactions:\n",
        "                raise ValueError('Invalid {} dimensions: length '\n",
        "                                 'must be equal to number of interactions'\n",
        "                                 .format(name))\n",
        "\n",
        "    def tocoo(self):\n",
        "        \"\"\"\n",
        "        Transform to a scipy.sparse COO matrix.\n",
        "        \"\"\"\n",
        "\n",
        "        row = self.user_ids\n",
        "        col = self.item_ids\n",
        "        data = self.ratings if self.ratings is not None else np.ones(len(self))\n",
        "\n",
        "        return sp.coo_matrix((data, (row, col)),\n",
        "                             shape=(self.num_users, self.num_items))\n",
        "\n",
        "    def tocsr(self):\n",
        "        \"\"\"\n",
        "        Transform to a scipy.sparse CSR matrix.\n",
        "        \"\"\"\n",
        "\n",
        "        return self.tocoo().tocsr()\n",
        "\n",
        "    def to_sequence(self, max_sequence_length=10, min_sequence_length=None, step_size=None):\n",
        "        \"\"\"\n",
        "        Transform to sequence form.\n",
        "        User-item interaction pairs are sorted by their timestamps,\n",
        "        and sequences of up to max_sequence_length events are arranged\n",
        "        into a (zero-padded from the left) matrix with dimensions\n",
        "        (num_sequences x max_sequence_length).\n",
        "        Valid subsequences of users' interactions are returned. For\n",
        "        example, if a user interacted with items [1, 2, 3, 4, 5], the\n",
        "        returned interactions matrix at sequence length 5 and step size\n",
        "        1 will be be given by:\n",
        "        .. code-block:: python\n",
        "           [[1, 2, 3, 4, 5],\n",
        "            [0, 1, 2, 3, 4],\n",
        "            [0, 0, 1, 2, 3],\n",
        "            [0, 0, 0, 1, 2],\n",
        "            [0, 0, 0, 0, 1]]\n",
        "        At step size 2:\n",
        "        .. code-block:: python\n",
        "           [[1, 2, 3, 4, 5],\n",
        "            [0, 0, 1, 2, 3],\n",
        "            [0, 0, 0, 0, 1]]\n",
        "        Parameters\n",
        "        ----------\n",
        "        max_sequence_length: int, optional\n",
        "            Maximum sequence length. Subsequences shorter than this\n",
        "            will be left-padded with zeros.\n",
        "        min_sequence_length: int, optional\n",
        "            If set, only sequences with at least min_sequence_length\n",
        "            non-padding elements will be returned.\n",
        "        step-size: int, optional\n",
        "            The returned subsequences are the effect of moving a\n",
        "            a sliding window over the input. This parameter\n",
        "            governs the stride of that window. Increasing it will\n",
        "            result in fewer subsequences being returned.\n",
        "        Returns\n",
        "        -------\n",
        "        sequence interactions: :class:`~SequenceInteractions`\n",
        "            The resulting sequence interactions.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.timestamps is None:\n",
        "            raise ValueError('Cannot convert to sequences, '\n",
        "                             'timestamps not available.')\n",
        "\n",
        "        if 0 in self.item_ids:\n",
        "            raise ValueError('0 is used as an item id, conflicting '\n",
        "                             'with the sequence padding value.')\n",
        "\n",
        "        if step_size is None:\n",
        "            step_size = max_sequence_length\n",
        "\n",
        "        # Sort first by user id, then by timestamp\n",
        "        sort_indices = np.lexsort((self.timestamps,\n",
        "                                   self.user_ids))\n",
        "\n",
        "        user_ids = self.user_ids[sort_indices]\n",
        "        item_ids = self.item_ids[sort_indices]\n",
        "\n",
        "        user_ids, indices, counts = np.unique(user_ids,\n",
        "                                              return_index=True,\n",
        "                                              return_counts=True)\n",
        "\n",
        "        num_subsequences = int(np.ceil(counts / float(step_size)).sum())\n",
        "\n",
        "        sequences = np.zeros((num_subsequences, max_sequence_length),\n",
        "                             dtype=np.int32)\n",
        "        sequence_users = np.empty(num_subsequences,\n",
        "                                  dtype=np.int32)\n",
        "        for i, (uid,\n",
        "                seq) in enumerate(_generate_sequences(user_ids,\n",
        "                                                      item_ids,\n",
        "                                                      indices,\n",
        "                                                      max_sequence_length,\n",
        "                                                      step_size)):\n",
        "            sequences[i][-len(seq):] = seq\n",
        "            sequence_users[i] = uid\n",
        "\n",
        "        if min_sequence_length is not None:\n",
        "            long_enough = sequences[:, -min_sequence_length] != 0\n",
        "            sequences = sequences[long_enough]\n",
        "            sequence_users = sequence_users[long_enough]\n",
        "\n",
        "        return (SequenceInteractions(sequences,\n",
        "                                     user_ids=sequence_users,\n",
        "                                     num_items=self.num_items))\n",
        "\n",
        "\n",
        "class SequenceInteractions(object):\n",
        "    \"\"\"\n",
        "    Interactions encoded as a sequence matrix.\n",
        "    Parameters\n",
        "    ----------\n",
        "    sequences: array of np.int32 of shape (num_sequences x max_sequence_length)\n",
        "        The interactions sequence matrix, as produced by\n",
        "        :func:`~Interactions.to_sequence`\n",
        "    num_items: int, optional\n",
        "        The number of distinct items in the data\n",
        "    Attributes\n",
        "    ----------\n",
        "    sequences: array of np.int32 of shape (num_sequences x max_sequence_length)\n",
        "        The interactions sequence matrix, as produced by\n",
        "        :func:`~Interactions.to_sequence`\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 sequences,\n",
        "                 user_ids=None, num_items=None):\n",
        "\n",
        "        self.sequences = sequences\n",
        "        self.user_ids = user_ids\n",
        "        self.max_sequence_length = sequences.shape[1]\n",
        "\n",
        "        if num_items is None:\n",
        "            self.num_items = sequences.max() + 1\n",
        "        else:\n",
        "            self.num_items = num_items\n",
        "\n",
        "    def __repr__(self):\n",
        "\n",
        "        num_sequences, sequence_length = self.sequences.shape\n",
        "\n",
        "        return ('<Sequence interactions dataset ({num_sequences} '\n",
        "                'sequences x {sequence_length} sequence length)>'\n",
        "                .format(\n",
        "                    num_sequences=num_sequences,\n",
        "                    sequence_length=sequence_length,\n",
        "                ))"
      ],
      "metadata": {
        "id": "knzWIB25sYPj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data helpers"
      ],
      "metadata": {
        "id": "ouVxAfnHsou_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def _index_or_none(array, shuffle_index):\n",
        "\n",
        "    if array is None:\n",
        "        return None\n",
        "    else:\n",
        "        return array[shuffle_index]\n",
        "\n",
        "def user_based_train_test_split(interactions,\n",
        "                                test_percentage=0.2,\n",
        "                                random_state=None):\n",
        "    \"\"\"\n",
        "    Split interactions between a train and a test set based on\n",
        "    user ids, so that a given user's entire interaction history\n",
        "    is either in the train, or the test set.\n",
        "    Parameters\n",
        "    ----------\n",
        "    interactions: :class:`spotlight.interactions.Interactions`\n",
        "        The interactions to shuffle.\n",
        "    test_percentage: float, optional\n",
        "        The fraction of users to place in the test set.\n",
        "    random_state: np.random.RandomState, optional\n",
        "        The random state used for the shuffle.\n",
        "    Returns\n",
        "    -------\n",
        "    (train, test): (:class:`spotlight.interactions.Interactions`,\n",
        "                    :class:`spotlight.interactions.Interactions`)\n",
        "         A tuple of (train data, test data)\n",
        "    \"\"\"\n",
        "\n",
        "    if random_state is None:\n",
        "        random_state = np.random.RandomState()\n",
        "\n",
        "    minint = np.iinfo(np.uint32).min\n",
        "    maxint = np.iinfo(np.uint32).max\n",
        "\n",
        "    seed = random_state.randint(minint, maxint, dtype=np.int64)\n",
        "\n",
        "    in_test = ((murmurhash3_32(interactions.user_ids,\n",
        "                               seed=seed,\n",
        "                               positive=True) % 100 /\n",
        "                100.0) <\n",
        "               test_percentage)\n",
        "    in_train = np.logical_not(in_test)\n",
        "\n",
        "    train = Interactions(interactions.user_ids[in_train],\n",
        "                         interactions.item_ids[in_train],\n",
        "                         ratings=_index_or_none(interactions.ratings,\n",
        "                                                in_train),\n",
        "                         timestamps=_index_or_none(interactions.timestamps,\n",
        "                                                   in_train),\n",
        "                         weights=_index_or_none(interactions.weights,\n",
        "                                                in_train),\n",
        "                         num_users=interactions.num_users,\n",
        "                         num_items=interactions.num_items)\n",
        "    test = Interactions(interactions.user_ids[in_test],\n",
        "                        interactions.item_ids[in_test],\n",
        "                        ratings=_index_or_none(interactions.ratings,\n",
        "                                               in_test),\n",
        "                        timestamps=_index_or_none(interactions.timestamps,\n",
        "                                                  in_test),\n",
        "                        weights=_index_or_none(interactions.weights,\n",
        "                                               in_test),\n",
        "                        num_users=interactions.num_users,\n",
        "                        num_items=interactions.num_items)\n",
        "\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "cCv6Xt-oszS7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ry2Plk7YtD5f"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from spotlight.sequence.implicit import ImplicitSequenceModel\n",
        "# from spotlight.cross_validation import user_based_train_test_split\n",
        "from spotlight.datasets.goodbooks import get_goodbooks_dataset\n",
        "from spotlight.datasets.amazon import get_amazon_dataset\n",
        "from spotlight.datasets.movielens import get_movielens_dataset\n",
        "# from spotlight.evaluation import sequence_mrr_score\n",
        "# from spotlight.torch_utils import set_seed\n",
        "# import hyperopt\n",
        "# from hyperopt import Trials, hp, fmin, STATUS_OK, STATUS_FAIL\n"
      ],
      "metadata": {
        "id": "x7oTVi8IVkKg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "L8-Y34xTWR7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fdvpMbIcWoxW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASETS = ['1m', '10m', 'amazon', 'goodbooks']\n",
        "MODELS = ['mlstm', 'lstm']"
      ],
      "metadata": {
        "id": "t8Ar2vLNWrcp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Args:\n",
        "    variant: str = DATASETS[0]\n",
        "    dataset: str = DATASETS[0]\n",
        "    num_trials: int = 100\n",
        "    model: str = MODELS[0]\n"
      ],
      "metadata": {
        "id": "Ly1dC1phYF_R"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = Args()"
      ],
      "metadata": {
        "id": "dN2Du6AFYjs-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_space = {\n",
        "    'batch_size': 240,\n",
        "    'learn_rate': 1.25e-2,\n",
        "    'l2': 5.90e-06,\n",
        "    'n_iter': 40,\n",
        "    'loss': 'adaptive_hinge',\n",
        "    'embedding_dim': 120,\n",
        "}\n",
        "\n",
        "space = common_space\n",
        "\n",
        "batch_size = int(space['batch_size'])\n",
        "learn_rate = space['learn_rate']\n",
        "loss = space['loss']\n",
        "n_iter = int(space['n_iter'])\n",
        "embedding_dim = int(space['embedding_dim'])\n",
        "l2 = space['l2']\n",
        "\n",
        "num_negative_samples = 5\n",
        "verbose = True\n",
        "seed=72"
      ],
      "metadata": {
        "id": "L2iWPo4al04d"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"device is {}!\".format(device))\n",
        "\n",
        "# Fix random_state\n",
        "torch.manual_seed(seed)\n",
        "random_state = np.random.RandomState(seed)\n",
        "\n",
        "max_sequence_length = 100\n",
        "min_sequence_length = 20\n",
        "step_size = max_sequence_length\n",
        "\n",
        "if args.dataset == 'amazon':\n",
        "    max_sequence_length = 50\n",
        "    min_sequence_length = 5\n",
        "    step_size = max_sequence_length\n",
        "    dataset = get_amazon_dataset()\n",
        "elif args.dataset == 'goodbooks':\n",
        "    dataset = get_goodbooks_dataset()\n",
        "else:\n",
        "    dataset = get_movielens_dataset(args.dataset.upper())\n",
        "\n",
        "args.variant = args.dataset\n",
        "train, rest = user_based_train_test_split(\n",
        "    dataset,\n",
        "    test_percentage=0.2,\n",
        "    random_state=random_state)\n",
        "test, valid = user_based_train_test_split(\n",
        "    rest,\n",
        "    test_percentage=0.5,\n",
        "    random_state=random_state)\n",
        "train = train.to_sequence(\n",
        "    max_sequence_length=max_sequence_length,\n",
        "    min_sequence_length=min_sequence_length,\n",
        "    step_size=step_size)\n",
        "test = test.to_sequence(\n",
        "    max_sequence_length=max_sequence_length,\n",
        "    min_sequence_length=min_sequence_length,\n",
        "    step_size=step_size)\n",
        "valid = valid.to_sequence(\n",
        "    max_sequence_length=max_sequence_length,\n",
        "    min_sequence_length=min_sequence_length,\n",
        "    step_size=step_size)\n",
        "\n",
        "print('model: {}, data: {}'.format(args.model, train))\n",
        "\n",
        "\n",
        "num_items=train.num_items\n",
        "\n",
        "representation = mLSTMNet(\n",
        "    num_items,\n",
        "    embedding_dim=embedding_dim)\n",
        "\n",
        "representation.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "                representation.parameters(),\n",
        "                weight_decay=l2,\n",
        "                lr=learn_rate\n",
        "            )\n",
        "\n",
        "sequences = train.sequences.astype(np.int64)\n",
        "\n",
        "check_input(sequences, num_items)\n",
        "\n",
        "\n",
        "start = time.perf_counter()\n",
        "try:\n",
        "    for epoch_num in range(n_iter):\n",
        "\n",
        "        sequences = shuffle(sequences,\n",
        "                            random_state=random_state)\n",
        "\n",
        "        sequences_tensor = torch.from_numpy(sequences)\n",
        "        sequences_tensor = sequences_tensor.to(device)\n",
        "\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        ## TODO: replace minibatch, shuffle with pytorch dataloader\n",
        "        ## TODO: do same with validation set\n",
        "        ## TODO: implement validation loss\n",
        "        for minibatch_num, batch_sequence in enumerate(minibatch(sequences_tensor,\n",
        "                                                                  batch_size=batch_size)):\n",
        "\n",
        "            sequence_var = batch_sequence\n",
        "\n",
        "            user_representation, _ = representation.user_representation(\n",
        "                sequence_var\n",
        "            )\n",
        "\n",
        "            positive_prediction = representation(user_representation,\n",
        "                                            sequence_var)\n",
        "\n",
        "\n",
        "            negative_prediction = get_multiple_negative_predictions(\n",
        "                sequence_var.size(),\n",
        "                user_representation,\n",
        "                net=representation,\n",
        "                n=num_negative_samples,\n",
        "                random_state=random_state)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss = adaptive_hinge_loss(positive_prediction,\n",
        "                                    negative_prediction,\n",
        "                                    mask=(sequence_var != PADDING_IDX))\n",
        "            # with torch.no_grad():\n",
        "            #     print(positive_prediction.shape)\n",
        "            #     print(user_representation.shape)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "        epoch_loss /= minibatch_num + 1\n",
        "\n",
        "        if verbose:\n",
        "            print('Epoch {}: loss {}'.format(epoch_num, epoch_loss))\n",
        "\n",
        "        if np.isnan(epoch_loss) or epoch_loss == 0.0:\n",
        "            raise ValueError('Degenerate epoch loss: {}'\n",
        "                              .format(epoch_loss))\n",
        "            \n",
        "\n",
        "    elapsed = time.perf_counter() - start\n",
        "\n",
        "    val_predictions = [-predict(representation, sequence, num_items) for sequence in valid.sequences[:, :-1]]\n",
        "    test_predictions = [-predict(representation, sequence, num_items) for sequence in valid.sequences[:, :-1]]\n",
        "\n",
        "    validation_mrr = sequence_mrr_score(\n",
        "        val_predictions,\n",
        "        valid.sequences[:, -1:],\n",
        "        exclude_preceding=True\n",
        "    ).mean()\n",
        "    test_mrr = sequence_mrr_score(\n",
        "        test_predictions,\n",
        "        test.sequences[:, -1:],\n",
        "        exclude_preceding=True\n",
        "    ).mean()\n",
        "except ValueError:\n",
        "    elapsed = time.perf_counter() - start\n",
        "    validation_mrr = 0.0\n",
        "    test_mrr = 0.0\n",
        "    print( {'loss': 0.0,\n",
        "            'status': 'FAIL',\n",
        "            'validation_mrr': 0.0,\n",
        "            'test_mrr': 0.0,\n",
        "            'elapsed': elapsed,\n",
        "            'hyper': space})\n",
        "\n",
        "\n",
        "print('MRR {} {}'.format(validation_mrr, test_mrr))\n",
        "\n",
        "if np.isnan(validation_mrr):\n",
        "    status = 'FAIL'\n",
        "else:\n",
        "    status = 'OK'\n",
        "\n",
        "print({'loss': -validation_mrr,\n",
        "        'status': status,\n",
        "        'validation_mrr': validation_mrr,\n",
        "        'test_mrr': test_mrr,\n",
        "        'elapsed': elapsed,\n",
        "        'hyper': space})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_CsF24QOXAr0",
        "outputId": "54e79a43-9bd9-4d6b-cf5a-a102b1b22452"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device is cuda!\n",
            "model: mlstm, data: <Sequence interactions dataset (9869 sequences x 100 sequence length)>\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n",
            "torch.Size([29, 100])\n",
            "torch.Size([29, 120, 100])\n",
            "Epoch 0: loss 0.8685762797083173\n",
            "torch.Size([240, 100])\n",
            "torch.Size([240, 120, 100])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-43b5400d62af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_var.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmKF3Xsq77j9",
        "outputId": "dac7ab3c-71ee-42c7-c4c2-5c83252d4c18"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([240, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_prediction.shape"
      ],
      "metadata": {
        "id": "BNYF_53cV1zs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f4f1a22-8022-4acf-d487-3d2021acf7a2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([240, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_representation.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLdLCBQx6hnF",
        "outputId": "ef6d6132-b0ea-4c33-c67a-549dab57b0bc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([240, 120, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2Wnw9b47lzk",
        "outputId": "eff97171-5d85-4054-9f20-5db2d2ab00c3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Sequence interactions dataset (1266 sequences x 100 sequence length)>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive_prediction[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-qhPF9K6vXF",
        "outputId": "b059f74e-9b6b-4b35-f811-18ad617befaa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-8.9897e-02,  8.3161e-01,  1.9490e+00,  1.6316e+00,  1.3015e+00,\n",
              "         1.4422e+00,  1.1159e+00,  1.2603e+00,  1.7145e+00,  7.8968e-01,\n",
              "         2.0591e+00,  1.1350e+00,  1.3748e+00,  1.0087e+00,  1.2679e+00,\n",
              "         1.0245e+00,  1.4537e+00,  1.0151e+00,  6.9320e-01,  8.3160e-01,\n",
              "         1.9181e-01,  7.3736e-01,  4.2203e-01,  5.1625e-01,  1.1969e+00,\n",
              "         1.8219e+00,  2.8332e+00,  1.2577e+00,  1.9564e+00,  9.8569e-01,\n",
              "         2.1016e+00,  2.4810e+00,  1.5026e+00,  2.1306e-01, -2.2869e-01,\n",
              "         1.8180e-01,  1.2645e+00, -8.7922e-02,  5.1651e-02,  1.7179e-01,\n",
              "         1.0909e-03,  4.3184e-01,  4.9093e-01,  1.1351e+00,  1.0513e+00,\n",
              "         1.4443e+00,  1.5232e+00,  1.1074e+00,  3.4924e-01, -1.6031e-01,\n",
              "         2.3361e-01, -2.0497e-01,  1.6062e-01,  2.2768e-01,  5.7506e-03,\n",
              "         1.9216e-01, -1.4283e-01,  1.9063e+00,  4.8740e-01,  2.6123e+00,\n",
              "         2.7423e+00,  3.3496e+00,  3.2346e+00,  3.0395e+00,  3.3466e+00,\n",
              "         3.6757e+00,  2.8670e+00,  3.5267e+00,  3.3422e+00,  2.9118e+00,\n",
              "         3.5190e+00,  2.7079e+00,  3.9202e+00,  2.8187e+00,  2.7032e+00,\n",
              "         3.2839e+00,  3.2357e+00,  2.3529e+00,  2.4468e+00,  3.4028e+00,\n",
              "         2.6719e+00,  3.3735e+00,  3.7526e+00,  3.8970e+00,  1.2708e+00,\n",
              "         2.2996e+00,  1.0503e+00,  2.6435e+00,  2.9915e+00,  1.8762e+00,\n",
              "         2.6088e+00,  2.3979e+00,  2.5953e+00,  1.8903e+00,  3.0575e+00,\n",
              "         3.2909e+00,  2.1614e+00,  3.6142e+00,  3.2454e+00,  1.6874e+00],\n",
              "       device='cuda:0', grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_var[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQkUfzir7zaM",
        "outputId": "389a7e30-9030-41ff-c779-f3e31f4d6c06"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 877,  516,    5,  232,  279, 1559,  355,  316,  436,  364,   69,  335,\n",
              "         220,  878, 2379,  940,  339, 1136,  552,  295,  975,  501,  312, 2160,\n",
              "          44,  351,  135,  264,  244,  446,   39,   51,  306, 2140,  673, 2251,\n",
              "         219,  224, 1674, 2629, 2579, 1236, 2628,  277,  310, 2515, 1121, 2578,\n",
              "         363, 2927, 2118, 1670, 1288, 2519, 2030, 2930,   45,  670, 1480,  125,\n",
              "         435,  214,   49,    6,  715,  210,  547,  218, 1110,  200,   98,  720,\n",
              "         133,  140,  216,  187,   93, 1081,  137,  217,    7,  152,  211,  157,\n",
              "         690,  273, 1285, 1229,   79, 1252,  749, 1155,   65,  209,  644,  647,\n",
              "         721,   63,  265,  729], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CVn13gOR8Pqz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}